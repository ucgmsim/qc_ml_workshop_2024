{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# QC Machine Learning Workshop Notebook\n",
    "[Github Repository](https://github.com/ucgmsim/qc_ml_workshop_2024)\n",
    "\n",
    "## Content\n",
    "This notebook is a general introduction to machine learning. It covers the following topics:\n",
    "- **Supervised Machine Learning Overview**\n",
    "- **Data Preparation**\n",
    "- **Model Fitting**\n",
    "- **Model Evaluation & Hyperparameters**\n",
    "\n",
    "#### Notebook Instructions\n",
    "Run cells containing code by clicking on them and hitting **Ctrl+Enter** or by Cell>Run Cells in the drop-down menu.\n",
    "\n",
    "#### Binder Timeout\n",
    "Binder has a timeout of 10 minutes. If you are inactive for more than 10 minutes, the session will be terminated. You can restart the session by clicking on the Binder link again. Changes you made to the notebook will be lost.\n",
    "\n",
    "### Figure Credit\n",
    "All figures are from the book \"Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili. Machine Learning with PyTorch and Scikit-Learn. Packt Publishing, 2022.\"  \n",
    " "
   ],
   "id": "25ed0b3b842e3af7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Machine Learning Overview\n",
    "\n",
    "\n",
    "### Types of Machine Learning\n",
    "<img src=\"resources/imgs/ml_types.png\" width=\"1000\"/>\n",
    "\n",
    "**Only looking at supervised ML in this workshop.**\n",
    "\n",
    "### Typical supervised ML workflow\n",
    "<img src=\"resources/imgs/ml_workflow.png\" width=\"1000\"/>\n",
    "\n",
    "### Classification: Predicting a class label  \n",
    "\n",
    "<img src=\"resources/imgs/ml_classification.png\" width=\"1000\">\n",
    "\n",
    "### Regression: Predicting a continuous value  \n",
    "\n",
    "<img src=\"resources/imgs/ml_regression.png\" width=\"1000\">"
   ],
   "id": "efb16ccb5af3dd58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Python Packages\n",
    "- **Pandas**: Data manipulation, [website](https://pandas.pydata.org/), [user guide](https://pandas.pydata.org/docs/user_guide/index.html)\n",
    "- **Numpy**: Numerical operations, [website](https://numpy.org/), [user guide](https://numpy.org/doc/stable/user/index.html)\n",
    "- **Scikit-learn**: Machine learning models, [website](https://scikit-learn.org/stable/index.html), [user guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- **Matplotlib**: Plotting, [website](https://matplotlib.org/), [user guide](https://matplotlib.org/stable/users/index)"
   ],
   "id": "7335f0975e6340d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Resources for further learning\n",
    "\n",
    "#### Free, Online\n",
    "- [Kaggle \"Courses\"](https://www.kaggle.com/learn)\n",
    "- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "\n",
    "#### Books\n",
    "- Machine Learning with Pytorch and Scitkit-learn, Sebastian Raschka\n",
    "- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, Aurélien Géron"
   ],
   "id": "fe62df5c65f8aa85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Problem Setup\n",
    "Before starting the process of developing a ML model it is important to consider the following:\n",
    "- **Problem Definition**: What are you trying to predict?\n",
    "- **Problem Type**: Classification or Regression\n",
    "- **Data Availability**: Do you have enough data and is it of sufficient quality?\n",
    "- **Evaluation Metric**: How will you evaluate the model performance? For a list of evaluation metrics see the [Scikit-learn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "### Workshop Problem\n",
    "- **Problem Definition**: Predict the weather pattern based on temperature, humidity and location type\n",
    "- **Problem Type**: Classification\n",
    "- **Data Availability**: Sufficient data available (see below)\n",
    "- **Evaluation Metric**: Accuracy\n"
   ],
   "id": "6e38fcc8eecdda34"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import notebook_code as nc\n",
    "df = nc.load_weather_data_pre(with_missing_values=True)\n",
    "df.head()"
   ],
   "id": "a0a65b80ef3b1385",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ML Terminology\n",
    "**Sample**: A single instance of data  \n",
    "**Feature**: An input variable used to make a prediction  \n",
    "**Label/Target**: The output variable we are trying to predict  \n",
    "**Model**: The algorithm used to make predictions  \n",
    "**Training**: The process of learning the model parameters from the data"
   ],
   "id": "35dcebd466befcf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Data Curation\n",
    "- Data collection\n",
    "- Data cleaning, e.g. handling of missing data\n",
    "\n",
    "### Data Exploration\n",
    "- Understanding data distributions\n",
    "- Identifying relationships between features and the target variable\n",
    "- Identifying anomalies and outliers\n",
    "- Identifying any potential biases in the data, e.g. imbalanced dataset\n",
    "- Identifying and selecting features\n",
    "\n",
    "**Example: Credit Card Fraud Label Imbalance**  \n",
    "<img src=\"resources/imgs/credit_card_fraud_imbalance.png\">  \n",
    "Figure from this [Kaggle notebook](https://www.kaggle.com/code/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets)\n",
    "\n",
    "### Data Preprocessing\n",
    "- Feature scaling\n",
    "- Encoding of categorial features\n",
    "- Encoding of target variable (if needed)\n",
    "\n",
    "Additional resources:\n",
    "- [Scikit-learn preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)"
   ],
   "id": "cf51358b72c37b44"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "import notebook_code as nc\n",
    "df = nc.load_weather_data_pre(with_missing_values=True)\n",
    "\n",
    "# Get size of the dataset\n",
    "print(df.shape)"
   ],
   "id": "23643a5b8f8f00ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ],
   "id": "f4b121e5928e6b58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get number of samples for each class\n",
    "df.groupby(\"target\").size()"
   ],
   "id": "8579018a1fd6df4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Missing Data\n",
    "\n",
    "How to handle:\n",
    "- Remove rows or features with missing data\n",
    "- Impute missing data, i.e. use interpolation techniques to infer missing values from the other samples, for example using the mean or median of the feature.\n",
    "\n",
    "Handling of missing data is dataset/problem specific!    "
   ],
   "id": "1f526d43f05801d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check for missing data\n",
    "# Note: This isn't always as easy, sometimes missing data is encoded as multiple specific values, e.g. N/A, -999, NA, etc.\n",
    "df.loc[df.isna().any(axis=1)].head(10)"
   ],
   "id": "877ed60042d8b619",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.isna().sum(axis=0)",
   "id": "74036b09b7151ea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop rows with missing data\n",
    "print(f\"Number of samples before dropping: {len(df)}\")\n",
    "df = df.dropna()\n",
    "print(f\"Number of samples after dropping: {len(df)}\")\n",
    "df.isna().sum(axis=0)"
   ],
   "id": "17f35ad7121c4b6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Almost all machine learning models require features to be features to be on the same scale, this prevents the model from being biased towards features with larger scales.  \n",
    "It's therefore good practice to always scale features before fitting a model."
   ],
   "id": "bb5cd16ad9a25f36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ml_nb_code import feature_scaling\n",
    "feature_scaling()"
   ],
   "id": "9402ce6976f04c3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Most common method is **Z-score standardisation**, which transforms the data to have zero mean and unit variance.  \n",
    "The formula for Z-score standardisation is:   \n",
    "$X_{std} = \\frac{X - \\mu} {\\sigma}$"
   ],
   "id": "7afac742f0d8fc2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply standardisation \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import notebook_code as nc\n",
    "df = nc.get_fs_data()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data (i.e. calculate the mean and standard deviation)\n",
    "scaler.fit(df[[\"x1\", \"x2\"]])\n",
    "\n",
    "# Transform the data\n",
    "df[[\"x1_std\", \"x2_std\"]] = scaler.transform(df[[\"x1\", \"x2\"]])\n",
    "\n",
    "print(f\"Mean: {df['x1_std'].mean()}, Std: {df['x1_std'].std()}\")\n",
    "print(f\"Mean: {df['x2_std'].mean()}, Std: {df['x2_std'].std()}\")\n",
    "\n",
    "# Plot the data\n",
    "nc.vis_scaling(df)"
   ],
   "id": "57bce32072d12cee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Categorial Features/Inputs\n",
    "\n",
    "Machine learning models require numerical inputs, so categorial features need to be encoded.  \n",
    "\n",
    "There are two types of categorial features:\n",
    "\n",
    "1. **Ordinal**: Features with an inherent order, e.g. small, medium, large\n",
    "2. **Nominal**: Features without an inherent order, e.g. red, green, blue"
   ],
   "id": "2510eaf8c9a8daf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example data\n",
    "import notebook_code as nc\n",
    "df = nc.load_cat_example()\n",
    "df"
   ],
   "id": "f1ffcc8d3d6980b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Ordinal Feature Mapping\n",
    "- Map the categories to numerical values, such that the numerical values reflect the order of the categories\n",
    "- Additionally, the numerical values should be close to the actual difference between the categories\n",
    "- This generally means that the mapping has to be defined manually\n",
    "- For example, small=0, medium=1, large=2"
   ],
   "id": "9846570624b2b32a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Map the ordinal feature\n",
    "size_mapping = {\n",
    "    \"M\": 1,\n",
    "    \"L\": 2,\n",
    "    \"XL\": 3\n",
    "}\n",
    "df[\"size_encoded\"] = df[\"size\"].map(size_mapping)\n",
    "df"
   ],
   "id": "7406e57a607981b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Nominal Feature Encoding\n",
    "- Cannot use the same approach as for ordinal features, as there is no inherent order!\n",
    "- For example setting green=1, red=2, blue=3 would imply that blue is larger than green\n",
    "- This is commonly addressed by using one-hot encoding, which creates a new binary feature for each category"
   ],
   "id": "3eff8dcd4a8720ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "# Fit the encoder to the data, (i.e. determine the categories)\n",
    "encoder.fit(df[[\"color\"]])\n",
    "# Get encoded feature column names\n",
    "encoded_columns = encoder.get_feature_names_out(['color'])\n",
    "# Transform the data\n",
    "df[encoded_columns] = encoder.transform(df[[\"color\"]])\n",
    "df"
   ],
   "id": "826b04a6c264512d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e28b616fdf0a9f67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Label encoding\n",
    "Similarly to features, ML models also require the target variable to be numerical. This is generally done using label encoding, i.e. assigning an integer value to each class.  \n",
    " The integer value assigned to the classes have no meaning, so can just enumerate the classes in the order they appear in the data. "
   ],
   "id": "8ba126cdda2bb3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load weather data\n",
    "import notebook_code as nc\n",
    "df = nc.load_weather_data_pre()\n",
    "df.head()"
   ],
   "id": "fcc80877f4feda67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the data\n",
    "df[\"target_encoded\"] = label_encoder.fit_transform(df[\"target\"])\n",
    "df"
   ],
   "id": "5236da5f3d3d2d91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inverse transform\n",
    "df[\"target_2\"] = label_encoder.inverse_transform(df[\"target_encoded\"])\n",
    "df"
   ],
   "id": "b806f278bf2a0360",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hands-On Data Preprocessing\n",
    "- Perform data preprocessing of the full weather dataset"
   ],
   "id": "3c09aec927e74ccf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "import notebook_code as nc\n",
    "df = nc.load_weather_data(with_missing_values=True, subset=True, features=[\"temperature\", \"humidity\", \"location_type\"])\n",
    "df.head()"
   ],
   "id": "18394101a63a23b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Hands-on section\n",
    "# 1) Handle missing data\n",
    "# 2) Standardise numerical features\n",
    "# 3) Encode categorial features\n",
    "# 4) Encode target variable\n",
    "\n",
    "### Your code here\n",
    "# TODO: Hide the solution!\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "\n",
    "# Drop rows with missing data\n",
    "print(f\"Number of samples before dropping: {len(df)}\")\n",
    "df = df.dropna()\n",
    "print(f\"Number of samples after dropping: {len(df)}\")\n",
    "df.isna().sum(axis=0)\n",
    "\n",
    "# Standardise numerical features\n",
    "numerical_features = [\"temperature\", \"humidity\"]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df[numerical_features])\n",
    "df[numerical_features] = scaler.transform(df[numerical_features])\n",
    "\n",
    "# Encode categorial features\n",
    "nominal_features = [\"location_type\"]\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(df[nominal_features])\n",
    "encoded_columns = encoder.get_feature_names_out(nominal_features)\n",
    "df[encoded_columns] = encoder.transform(df[nominal_features])\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"target_encoded\"] = label_encoder.fit_transform(df[\"target\"])\n",
    "\n",
    "df.head()"
   ],
   "id": "cb5f7455b75aaac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Fitting\n",
    "In this notebook we will focus solely on decision tree models, however there are many other model types available.\n",
    "\n",
    "List of most common ML models according to ChatGPT:\n",
    "> The most commonly used supervised machine learning models are:\n",
    "> \n",
    "> 1. **Linear Regression**: Used for predicting continuous values by modeling the relationship between input features and the target variable.\n",
    "> \n",
    "> 2. **Logistic Regression**: Used for binary classification tasks by estimating the probability that an instance belongs to a particular class.\n",
    "> \n",
    "> 3. **Decision Trees**: A versatile model that splits data into subsets based on feature values, used for both classification and regression tasks.\n",
    "> \n",
    "> 4. **Random Forest**: An ensemble of decision trees that improves performance by averaging the predictions of multiple trees to reduce overfitting.\n",
    "> \n",
    "> 5. **Support Vector Machines (SVM)**: A powerful model for classification tasks that finds the optimal hyperplane to separate different classes.\n",
    "> \n",
    "> 6. **k-Nearest Neighbors (k-NN)**: A simple classification model that assigns labels based on the majority class of the k-nearest neighbors in the training data.\n",
    "> \n",
    "> 7. **Naive Bayes**: A probabilistic classifier based on Bayes' theorem, assuming independence between features, often used for text classification.\n",
    "> \n",
    "> 8. **Gradient Boosting Machines (GBM)**: An ensemble model that builds trees sequentially, with each new tree correcting errors made by the previous ones (e.g., XGBoost, LightGBM, CatBoost).\n",
    "> \n",
    "> 9. **Neural Networks**: Particularly effective for complex tasks like image recognition and natural language processing, neural networks can model intricate patterns by learning from multiple layers of abstraction.\n",
    "> \n",
    "> 10. **k-Nearest Neighbors (k-NN)**: A non-parametric model used for both classification and regression, which predicts the output based on the closest training examples in the feature space.\n",
    "> \n",
    "> These models are widely used due to their effectiveness across various domains and their ability to handle different types of data and tasks.\n",
    "\n",
    "The scikit-learn library has implementation for almost all of these, see the [documentation](https://scikit-learn.org/stable/user_guide.html) for more details."
   ],
   "id": "d3f90c2619c3aa76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Decision Tree  \n",
    "<img src=\"resources/imgs/ml_decision_tree_example.png\">"
   ],
   "id": "19737c962163e713"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decision Tree - Manual Fitting",
   "id": "11b5ed885f5bf781"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import man_decision_tree as mdt\n",
    "mdt.run_man_decision_tree()"
   ],
   "id": "b585b5ad169a47e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decision Tree - Fitting",
   "id": "21be33548d7ff04f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the pre-processed data\n",
    "import notebook_code as nc\n",
    "df, features, label_encoder = nc.load_weather_data(subset=True, pre_process=True, features=[\"temperature\", \"humidity\", \"location_type\"])\n",
    "df = df[features + [\"target_encoded\"]]\n",
    "df"
   ],
   "id": "68f84c4c2c8742d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit a decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(df[features], df[\"target_encoded\"])\n",
    "\n",
    "# Get model predictions\n",
    "y_pred = clf.predict(df[features])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(df[\"target_encoded\"], y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "id": "7a5d59b9f9a2ce5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(16, 10))\n",
    "plot_tree(clf, filled=True, impurity=False, feature_names=features, class_names=label_encoder.inverse_transform(clf.classes_))\n",
    "plt.show()"
   ],
   "id": "462d71679450853c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note: Left corresponds to True, Right corresponds to False",
   "id": "8968faeac046845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Plot decision boundary\n",
    "nc.plot_decision_boundaries(df, clf, features, label_encoder)"
   ],
   "id": "431348a24a83fcb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Overfitting and Underfitting",
   "id": "5d71049b61a85eaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import notebook_code as nc\n",
    "nc.linear_regression_fitting_example()"
   ],
   "id": "ad252013a0bc7576",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## How to tell if a model is overfitting or underfitting? - Model Evaluation\n",
    "- Evaluate model performance on unseen data"
   ],
   "id": "643cb681b0d34fcc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import notebook_code as nc\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Data loading\n",
    "features = [\"temperature\", \"humidity\", \"location_type\"]\n",
    "df = nc.load_weather_data(subset=True, features=features)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[features],\n",
    "                                                  df[\"target\"],\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "val_df = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "print(f\"Number of samples before splitting: {len(df)}\")\n",
    "print(f\"Number of training samples: {len(train_df)}\")\n",
    "print(f\"Number of validation samples: {len(val_df)}\")"
   ],
   "id": "121de60bf13c5063",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Pre-processing\n",
    "# Standardise numerical features\n",
    "numerical_features = [\"temperature\", \"humidity\"]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_df[numerical_features])\n",
    "train_df[numerical_features] = scaler.transform(train_df[numerical_features])\n",
    "val_df[numerical_features] = scaler.transform(val_df[numerical_features])\n",
    "\n",
    "# Encode categorial features\n",
    "nominal_features = [\"location_type\"]\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoder.fit(train_df[nominal_features])\n",
    "encoded_columns = encoder.get_feature_names_out(nominal_features)\n",
    "train_df[encoded_columns] = encoder.transform(train_df[nominal_features])\n",
    "val_df[encoded_columns] = encoder.transform(val_df[nominal_features])\n",
    "features = numerical_features + list(encoded_columns)\n",
    "\n",
    "# Encode target variable\n",
    "label_encoder = LabelEncoder()\n",
    "train_df[\"target_encoded\"] = label_encoder.fit_transform(train_df[\"target\"])\n",
    "val_df[\"target_encoded\"] = label_encoder.transform(val_df[\"target\"])"
   ],
   "id": "ed99715366e6744",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(train_df[features], train_df[\"target_encoded\"])\n",
    "\n",
    "# Get model predictions\n",
    "train_y_pred = clf.predict(train_df[features])\n",
    "val_y_pred = clf.predict(val_df[features])\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(train_df[\"target_encoded\"], train_y_pred)\n",
    "val_accuracy = accuracy_score(val_df[\"target_encoded\"], val_y_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Plot the decision boundary\n",
    "nc.plot_decision_boundaries(train_df, clf, features, label_encoder, val_df=val_df, show_train=False)"
   ],
   "id": "1914abd26d2e05fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Holdout Method\n",
    "- Split available (labelled) data into a training and validation set\n",
    "- Generally split data 80% for training and 20% for validation\n",
    "- Scikit-learn function: [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ],
   "id": "508c10daa32a7187"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## How To Address Underfitting/Overfitting?\n",
    "\n",
    "- Collect more data\n",
    "- Data augmentation\n",
    "- Feature engineering\n",
    "- Feature selection\n",
    "- Increase/Reduce model complexity\n",
    "- Decrease/Increase applied regularization\n",
    "- Explore different model types"
   ],
   "id": "7ab62fb2367a052f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters\n",
    "Parameters that are set before the training of the model that control the learning process.  \n",
    "Unlike model parameters, hyperparameters are not learned during training, instead these are set by the user.  \n",
    "They have a significant impact on the model performance and can be used to address overfitting/underfitting.    \n",
    "  \n",
    "They are generally model specific, for example the hyperparameters for a Decision Tree model are different from those of a Neural Network model.\n",
    "\n",
    "Examples for a Decision Tree:\n",
    "- Maximum depth of the tree\n",
    "- Minimum number of samples required to split a node\n",
    "- Minimum number of samples required at each leaf node\n",
    "- For a full list see the Decision Tree [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ],
   "id": "7dabc1b5d7888798"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Hyperparameter Example\n",
    "import notebook_code as nc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Data loading\n",
    "features = [\"temperature\", \"humidity\", \"location_type\"]\n",
    "df = nc.load_weather_data(subset=True, features=features)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(df[features],\n",
    "                                                  df[\"target\"],\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=42)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "val_df = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "print(f\"Number of samples before splitting: {len(df)}\")\n",
    "print(f\"Number of training samples: {len(train_df)}\")\n",
    "print(f\"Number of validation samples: {len(val_df)}\")\n",
    "\n",
    "# Run the pre-processing\n",
    "train_df, val_df, features, label_encoder = nc.run_weather_preprocess(train_df, val_df)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(max_depth=4)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(train_df[features], train_df[\"target_encoded\"])\n",
    "\n",
    "# Get model predictions\n",
    "train_y_pred = clf.predict(train_df[features])\n",
    "val_y_pred = clf.predict(val_df[features])\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(train_df[\"target_encoded\"], train_y_pred)\n",
    "val_accuracy = accuracy_score(val_df[\"target_encoded\"], val_y_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ],
   "id": "73064cb9e7b1eef9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16, 10))\n",
    "plot_tree(clf, filled=True, impurity=False, feature_names=features, class_names=label_encoder.inverse_transform(clf.classes_))\n",
    "plt.show()"
   ],
   "id": "401839a3eabfa7be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Interactive Hyperparameter Example\n",
    "### Task/Questions:\n",
    "# - What do these hyperparameters do? See the documentation, https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "# - How do they impact the model?\n",
    "# - What are the differences and commonalities between the three hyperparameters?\n",
    "# - What are some other hyperparameters that could be used?\n",
    "# - What is the best hyperparameter combination?\n",
    "import notebook_code as nc\n",
    "nc.manual_hyperparam_tuning()"
   ],
   "id": "5863d7c2cbf1f7f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Complete model evaluation strategy\n",
    "- **Training dataset**: Used to train the model\n",
    "- **Validation dataset**: Used to evaluate the model performance during hyperparameter tuning\n",
    "- **Test dataset**: Used to evaluate the final model performance\n",
    "\n",
    "<img src=\"resources/imgs/ml_train_val_test.png\">"
   ],
   "id": "9541e40fd7dd455f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Variation in holdout runs\n",
    "import notebook_code as nc\n",
    "nc.holdout_variations()"
   ],
   "id": "c5651c8f72e5c1f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### K-Fold Cross Validation\n",
    "- Provides a more robust estimate of the model performance\n",
    "- Gives indication on uncertainty in the model performance\n",
    "- Also supported by scikit-learn: [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "\n",
    "<img src=\"resources/imgs/ml_cross_val.png\" width=\"1000\"/>"
   ],
   "id": "1835713521d9109c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Cross Validation Example\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import notebook_code as nc\n",
    "\n",
    "# Get the dataset\n",
    "features = [\"temperature\", \"humidity\", \"location_type\"]\n",
    "df, features, label_encoder = nc.load_weather_data(subset=True, features=features, pre_process=True)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Use cross-validation to evaluate the model\n",
    "cv_scores = cross_val_score(clf, df[features], df[\"target_encoded\"], cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f\"Cross-validation scores: {', '.join([f'{cur_score:.3f}' for cur_score in cv_scores])}\")\n",
    "print(f\"Mean cross-validation score: {cv_scores.mean():.2f}\")\n",
    "print(f\"Standard deviation of cross-validation scores: {cv_scores.std():.2f}\")"
   ],
   "id": "16e6fb586fd934a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## How to select the \"best\" hyperparameter values?\n",
    "\n",
    "### Hyperparameter tuning\n",
    "- Basic idea: Test different hyperparameter value combinations, evaluate their performance and select the best one\n",
    "- Lots of different methods available, see the [Scikit-learn User Guide](https://scikit-learn.org/stable/modules/grid_search.html) for more details\n",
    "\n",
    "##### Grid Search\n",
    "Determines the best hyperparameters by exhaustively searching through a specified parameter grid   \n",
    "\n",
    "##### Random Search\n",
    "Determines the best hyperparameters by randomly sampling from a specified parameter grid\n",
    "\n",
    "<img src=\"resources/imgs/ml_hyper_tuning.png\" width=\"1000\"/>"
   ],
   "id": "f1f4ebc9400b8ecb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Grid Search Example\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import notebook_code as nc\n",
    "\n",
    "features = [\"temperature\", \"humidity\", \"location_type\"]\n",
    "df, features, label_encoder = nc.load_weather_data(subset=True, features=features, pre_process=True)\n",
    "\n",
    "# Create the classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for the search\n",
    "param_grid = {\n",
    "    'max_depth': [2, 4, 8],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "# Perform the grid search using cross-validation\n",
    "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(df[features], df[\"target_encoded\"])\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ],
   "id": "6ae86de41a604edb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a dataframe with the results\n",
    "results = []\n",
    "for cur_param in param_grid.keys():\n",
    "    results.append(pd.DataFrame(grid_search.cv_results_)[f\"param_{cur_param}\"])\n",
    "\n",
    "results_df = pd.concat(results, axis=1)\n",
    "results_df[\"mean_val_score\"] = grid_search.cv_results_[\"mean_test_score\"]\n",
    "results_df[\"std_val_score\"] = grid_search.cv_results_[\"std_test_score\"]"
   ],
   "id": "2fdb71a37a97736e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results_df.sort_values(\"mean_val_score\", ascending=False).head(10)",
   "id": "e71b4574f31470f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Roadmap\n",
    "\n",
    "<img src=\"resources/imgs/ml_roadmap.png\">"
   ],
   "id": "4a0bc391d324225b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Full Hands-On Example\n",
    "- Apply the above concepts to a new problem\n",
    "- Below are multiple options, if most of the content above was new for you then we recommend the first option.\n",
    "\n",
    "Also feel free to any model type you like, not just Decision Trees.\n",
    "For a list of supported models by scikit-learn see the [documentation](https://scikit-learn.org/stable/user_guide.html).  \n",
    "The random forest model is a good choice if you want to try something new. It is an ensemble model that uses multiple decision trees to improve performance."
   ],
   "id": "ebbe4ad0d0734501"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Full Weather Dataset\n",
    "The examples above only used a subset of the weather pattern classification dataset. See of you can apply the concepts to the full dataset and improve the model performance.  \n",
    "[Dataset description](https://www.kaggle.com/datasets/nikhil7280/weather-type-classification)"
   ],
   "id": "94106b7092a4e897"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Loading of the full weather dataset\n",
    "import notebook_code as nc\n",
    "df = nc.load_weather_data()"
   ],
   "id": "253344f813b49cb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "f05a8c3888c5192e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Concrete Compressive Strength Dataset (Regression)\n",
    "[Dataset description](https://archive.ics.uci.edu/dataset/165/concrete+compressive+strength)\n",
    " "
   ],
   "id": "9beff778a73c7c69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loading of the dataset\n",
    "df = pd.read_csv(\"resources/concrete.csv\")"
   ],
   "id": "b20cc4f5c904ef1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "c460cbe87ed8f1a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Heart Disease Dataset (Classification)\n",
    "Predict the presence of heart disease based on various features.\n",
    "[Dataset details](https://archive.ics.uci.edu/dataset/45/heart+disease)"
   ],
   "id": "ddb82f0fe4de3993"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loading of the dataset\n",
    "import notebook_code as nc\n",
    "df = nc.load_heart_df() "
   ],
   "id": "af9968c4c479e9cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.head()",
   "id": "cd30127123dbff65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### New Zealand GM Prediction (Regression)\n",
    "Use historical earthquakes from New Zealand to develop a ML-based GMM."
   ],
   "id": "679c26ee0c86e551"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loading of the dataset\n",
    "df = pd.read_csv(\"resources/ground_motion_im_table_rotd50_flat.csv\", dtype={\"evid\": str})"
   ],
   "id": "61b3793cb94f1635",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "52ee795bf499507c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
