{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# QC Machine Learning Workshop Notebook\n",
    "\n",
    "## Content\n",
    "This notebook is a general introduction to machine learning. It covers the following topics:\n",
    "- **Supervised Machine Learning Overview & Jargon**\n",
    "- **Data Preprocessing**\n",
    "    - Missing data\n",
    "    - Input standardisation\n",
    "    - Categorial inputs\n",
    "    - Label encoding\n",
    "- **Model Fitting**\n",
    "    - Decision tree fitting\n",
    "    - Overfitting/Underfitting\n",
    "- **Model Evaluation & Hyperparameters**\n",
    "    - Train/Validation Split\n",
    "    - Cross Validation\n",
    "- **Hyperparameters**\n",
    "    - Hyperparameters\n",
    "    - Hyperparameter Tuning\n",
    "\n",
    "#### Notebook Instructions\n",
    "Run cells containing code by clicking on them and hitting **Ctrl+Enter** or by Cell>Run Cells in the drop-down menu.\n",
    "\n",
    "#### Binder Timeout\n",
    "Binder has a timeout of 10 minutes. If you are inactive for more than 10 minutes, the session will be terminated. You can restart the session by clicking on the Binder link again. Changes you made to the notebook will be lost.\n",
    "\n",
    "### Github Repository\n",
    "The code for this notebook can be found in the following [repository](https://github.com/ucgmsim/qc_ml_workshop_2024).\n",
    "\n",
    "### Figure Credit\n",
    "All figures are from the book \"Sebastian Raschka, Yuxi (Hayden) Liu, and Vahid Mirjalili. Machine Learning with PyTorch and Scikit-Learn. Packt Publishing, 2022.\"  \n",
    " "
   ],
   "id": "25ed0b3b842e3af7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Machine Learning Overview\n",
    "\n",
    "\n",
    "### Types of Machine Learning\n",
    "<img src=\"resources/imgs/ml_types.png\" width=\"1000\"/>\n",
    "\n",
    "**Only looking at supervised ML in this workshop.**\n",
    "\n",
    "### Typical supervised ML workflow\n",
    "<img src=\"resources/imgs/ml_workflow.png\" width=\"1000\"/>\n",
    "\n",
    "### Classification vs Regression\n",
    "Classification: Predicting a class label  \n",
    "\n",
    "<img src=\"resources/imgs/ml_classification.png\" width=\"1000\">\n",
    "\n",
    "Regression: Predicting a continuous value  \n",
    "\n",
    "<img src=\"resources/imgs/ml_regression.png\" width=\"1000\">\n",
    "\n",
    "### Jargon\n",
    "<img src=\"resources/imgs/ml_jargon.png\" width=\"1000\">\n",
    "\n",
    "**Sample**: A single instance of data  \n",
    "**Feature**: An input variable used to make a prediction  \n",
    "**Label**: The output variable we are trying to predict  \n",
    "**Model**: The algorithm used to make predictions  \n",
    "**Training**: The process of learning the model from the data"
   ],
   "id": "efb16ccb5af3dd58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Python Packages\n",
    "- **Pandas**: Data manipulation, [website](https://pandas.pydata.org/), [user guide](https://pandas.pydata.org/docs/user_guide/index.html)\n",
    "- **Numpy**: Numerical operations, [website](https://numpy.org/), [user guide](https://numpy.org/doc/stable/user/index.html)\n",
    "- **Scikit-learn**: Machine learning models, [website](https://scikit-learn.org/stable/index.html), [user guide](https://scikit-learn.org/stable/user_guide.html)\n",
    "- **Matplotlib**: Plotting, [website](https://matplotlib.org/), [user guide](https://matplotlib.org/stable/users/index)"
   ],
   "id": "7335f0975e6340d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Resources for further learning\n",
    "\n",
    "#### Free, Online\n",
    "- [Kaggle \"Courses\"](https://www.kaggle.com/learn)\n",
    "\n",
    "#### Books\n",
    "- Machine Learning with Pytorch and Scitkit-learn, Sebastian Raschka\n",
    "- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, Aurélien Géron"
   ],
   "id": "fe62df5c65f8aa85"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# General imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ],
   "id": "403eb88cec603552",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Why?\n",
    "- Garbage in, garbage out\n",
    "- Data needs to be in a format that the model can understand\n",
    "\n",
    "Additional resources:\n",
    "- [Scikit-learn preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)"
   ],
   "id": "cf51358b72c37b44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Missing Data\n",
    "Important to check if there is missing data and handle this. \n",
    "Potential to break models, silently and non-silently.\n",
    "\n",
    "How to handle:\n",
    "- Remove rows or features with missing data\n",
    "- Impute missing data, i.e. use interpolation techniques to infer missing values from the other samples, for example using the mean or median of the feature.\n",
    "\n",
    "Handling of missing data is dataset/problem specific!    "
   ],
   "id": "1f526d43f05801d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ml_nb_code import get_nan_example\n",
    "df = get_nan_example()\n",
    "df"
   ],
   "id": "f02f2bc42fc84f26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop rows with missing data\n",
    "df = df.dropna()\n",
    "df.loc[df.isna().any(axis=1)]"
   ],
   "id": "19aba4c4921f47b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "17f35ad7121c4b6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Standardization/Normalization\n",
    "\n",
    "Almost all machine learning models require features to be features to be on the same scale, this prevents the model from being biased towards features with larger scales."
   ],
   "id": "bb5cd16ad9a25f36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ml_nb_code import feature_scaling\n",
    "feature_scaling()"
   ],
   "id": "9402ce6976f04c3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Two common methods to scale features:\n",
    "\n",
    "#### Min-Max Scaling\n",
    "Transforms the data to be between a lower bound $x_min$ and upper bound $x_max$  \n",
    "$X_{norm} = \\frac{X - X_{min}} {X_{max} - X_{min}}$\n",
    "\n",
    "#### Standardisation\n",
    "Transforms the data to have zero mean and unit variance   \n",
    "$X_{std} = \\frac{X - \\mu} {\\sigma}$"
   ],
   "id": "7afac742f0d8fc2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Apply standardisation \n",
    "from ml_nb_code import feature_scaling_example\n",
    "feature_scaling_example()"
   ],
   "id": "57bce32072d12cee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Implementation",
   "id": "a25f45f235a48881"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Manually\n",
    "from ml_nb_code import get_fs_data\n",
    "df = get_fs_data()\n",
    "\n",
    "df[\"x1_norm\"] = (df[\"x1\"] - df[\"x1\"].mean()) / df[\"x1\"].std()\n",
    "df[\"x2_norm\"] = (df[\"x2\"] - df[\"x2\"].mean()) / df[\"x2\"].std()\n",
    "\n",
    "print(f\"Mean: {df['x1_norm'].mean()}, Std: {df['x1_norm'].std()}\")\n",
    "print(f\"Mean: {df['x2_norm'].mean()}, Std: {df['x2_norm'].std()}\")"
   ],
   "id": "5bda004f49a5880",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Using sklearn\n",
    "from ml_nb_code import get_fs_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = get_fs_data()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data (i.e. calculate the mean and standard deviation)\n",
    "scaler.fit(df[[\"x1\", \"x2\"]])\n",
    "# Transform the data\n",
    "df[[\"x1_norm\", \"x2_norm\"]] = scaler.transform(df[[\"x1\", \"x2\"]])\n",
    "\n",
    "# Can combine the fit and transform steps\n",
    "# df[[\"x1_norm\", \"x2_norm\"]] = scaler.fit_transform(df[[\"x1\", \"x2\"]])\n",
    "\n",
    "print(f\"Mean: {df['x1_norm'].mean()}, Std: {df['x1_norm'].std()}\")\n",
    "print(f\"Mean: {df['x2_norm'].mean()}, Std: {df['x2_norm'].std()}\")"
   ],
   "id": "1900fac8bf0bb2f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Categorial Features/Inputs\n",
    "\n",
    "Machine learning models require numerical inputs, so categorial features need to be encoded.  \n",
    "There are multiple ways to do this, one of the most common approaches is one-hot encoding."
   ],
   "id": "2510eaf8c9a8daf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load some example data\n",
    "from ml_nb_code import get_safe_unsafe_data\n",
    "df = get_safe_unsafe_data()\n",
    "df"
   ],
   "id": "94cb168d0ef0c8b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(df[['material_type']])\n",
    "\n",
    "# Create a new dataframe with the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['material_type']), dtype=int)\n",
    "# Concatenate the original and encoded dataframes\n",
    "result_df = pd.concat([df, encoded_df], axis=1)\n",
    "result_df"
   ],
   "id": "53d8bfb3107e4c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Label encoding\n",
    "Similarly to features, ML models also require the target variable to be numerical. This is generally done using label encoding."
   ],
   "id": "8ba126cdda2bb3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = get_safe_unsafe_data()\n",
    "df"
   ],
   "id": "fcc80877f4feda67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the data\n",
    "df[\"safe_encoded\"] = label_encoder.fit_transform(df[\"safe\"])\n",
    "df"
   ],
   "id": "5236da5f3d3d2d91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inverse transform\n",
    "df[\"safe_2\"] = label_encoder.inverse_transform(df[\"safe_encoded\"])\n",
    "df"
   ],
   "id": "b806f278bf2a0360",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Fitting\n",
    "Now that we have preprocessed the data, we can fit a model to it.\n",
    "In this notebook we will focus solely on decision tree models, however there are many other model types available.\n",
    "\n",
    "List of most common ML models according to ChatGPT:\n",
    "> The most commonly used supervised machine learning models are:\n",
    "> \n",
    "> 1. **Linear Regression**: Used for predicting continuous values by modeling the relationship between input features and the target variable.\n",
    "> \n",
    "> 2. **Logistic Regression**: Used for binary classification tasks by estimating the probability that an instance belongs to a particular class.\n",
    "> \n",
    "> 3. **Decision Trees**: A versatile model that splits data into subsets based on feature values, used for both classification and regression tasks.\n",
    "> \n",
    "> 4. **Random Forest**: An ensemble of decision trees that improves performance by averaging the predictions of multiple trees to reduce overfitting.\n",
    "> \n",
    "> 5. **Support Vector Machines (SVM)**: A powerful model for classification tasks that finds the optimal hyperplane to separate different classes.\n",
    "> \n",
    "> 6. **k-Nearest Neighbors (k-NN)**: A simple classification model that assigns labels based on the majority class of the k-nearest neighbors in the training data.\n",
    "> \n",
    "> 7. **Naive Bayes**: A probabilistic classifier based on Bayes' theorem, assuming independence between features, often used for text classification.\n",
    "> \n",
    "> 8. **Gradient Boosting Machines (GBM)**: An ensemble model that builds trees sequentially, with each new tree correcting errors made by the previous ones (e.g., XGBoost, LightGBM, CatBoost).\n",
    "> \n",
    "> 9. **Neural Networks**: Particularly effective for complex tasks like image recognition and natural language processing, neural networks can model intricate patterns by learning from multiple layers of abstraction.\n",
    "> \n",
    "> 10. **k-Nearest Neighbors (k-NN)**: A non-parametric model used for both classification and regression, which predicts the output based on the closest training examples in the feature space.\n",
    "> \n",
    "> These models are widely used due to their effectiveness across various domains and their ability to handle different types of data and tasks.\n",
    "\n",
    "The scikit-learn library has implementation for almost all of these, see the [documentation](https://scikit-learn.org/stable/user_guide.html) for more details."
   ],
   "id": "d3f90c2619c3aa76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decision Tree - Manual Fitting",
   "id": "11b5ed885f5bf781"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Todo: Simplify this, add more example data, add tasks/questions\n",
    "from ml_nb_code import decision_tree\n",
    "decision_tree()"
   ],
   "id": "b585b5ad169a47e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "For the rest of this notebook we will use the heart disease dataset from the UCI Machine Learning Repository. You can find additional information about the dataset [here](https://archive.ics.uci.edu/ml/datasets/heart+Disease).\n",
    "\n",
    "The aim of this dataset is to predict whether a patient has heart disease or not based on a number of features. The target variable is binary, i.e. 0 for no heart disease and 1 for heart disease.  \n",
    "There are a total of 14 features in the dataset, some of which are categorical and some numerical.   \n",
    "For simplicity, we will only utilise three of these features: `thalach` (maximum heart rate achieved), `oldpeak` (ST depression induced by exercise relative to rest), and `thal` (thalassemia)."
   ],
   "id": "dfbd9047f29ae7df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Pre-processing (Hands on)\n",
    "- Hands on data pre-processing of the heart disease dataset\n"
   ],
   "id": "f8c2278c03ba149c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Import relevant libraries and load the data\n",
    "from ml_nb_code import get_heart_df\n",
    "\n",
    "heart_df = get_heart_df(features=[\"thalach\", \"oldpeak\", \"thal\"])\n",
    "heart_df"
   ],
   "id": "79373e65cc9ec9c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Hands-On - Prepare the data\n",
    "\n",
    "## ----------- Solution ----------- (Todo:Remove)\n",
    "# Import the necessary libraries\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check for missing values\n",
    "print(heart_df.isna().sum())\n",
    "\n",
    "# Normalise the features\n",
    "numerical_features = [\"thalach\", \"oldpeak\"]\n",
    "std_scaler = StandardScaler()\n",
    "heart_df[numerical_features] = std_scaler.fit_transform(heart_df[numerical_features])\n",
    "\n",
    "# Encode the categorical features\n",
    "categorical_features = [\"thal\"]\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_data = encoder.fit_transform(heart_df[categorical_features])\n",
    "heart_df[encoder.get_feature_names_out()] = encoded_data\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "heart_df[\"target_encoded\"] = label_encoder.fit_transform(heart_df[\"target\"])\n",
    "\n",
    "features = numerical_features + list(encoder.get_feature_names_out())\n"
   ],
   "id": "5fa14e3d71184cb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "heart_df",
   "id": "1511cb94bfc7384b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decision Tree - Fitting",
   "id": "21be33548d7ff04f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit a decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(heart_df[features], heart_df[\"target_encoded\"])\n",
    "\n",
    "# Get model predictions\n",
    "y_pred = clf.predict(heart_df[features])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(heart_df[\"target_encoded\"], y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "id": "7a5d59b9f9a2ce5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(clf, filled=True, impurity=False, feature_names=features, class_names=label_encoder.inverse_transform(clf.classes_))\n",
    "plt.show()"
   ],
   "id": "462d71679450853c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note: Left corresponds to True, Right corresponds to False",
   "id": "8968faeac046845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot decision boundaries\n",
    "from ml_nb_code import plot_decision_boundary_heart\n",
    "plot_decision_boundary_heart(heart_df, clf, features)"
   ],
   "id": "431348a24a83fcb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Overfitting and Underfitting",
   "id": "5d71049b61a85eaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ml_nb_code import linear_regression_fitting_example\n",
    "linear_regression_fitting_example()"
   ],
   "id": "ad252013a0bc7576",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation\n",
    "- Evaluate model performance on unseen data\n",
    "\n",
    "### Holdout Method\n",
    "- Split available (labelled) data into a training and validation set (also sometimes called testing set)\n",
    "- Generally split data 80% for training and 20% for validation\n",
    "- Scikit-learn has a function for this: [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "\n",
    "### Cross Validation\n",
    "- Allows for better estimation of model performance\n",
    "- Gives indication on uncertainty in the model performance\n",
    "- Also supported by scikit-learn: [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "\n",
    "<img src=\"resources/imgs/ml_cross_val.png\" width=\"1000\"/>"
   ],
   "id": "1835713521d9109c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Holdout Example\n",
    "from ml_nb_code import get_prepped_heart_df\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "seed = 42\n",
    "\n",
    "heart_df, feature_keys, _ = get_prepped_heart_df()\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(heart_df[feature_keys],\n",
    "                                                  heart_df[\"target_encoded\"],\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=seed)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "val_df = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=seed)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(train_df[feature_keys], train_df[\"target_encoded\"])\n",
    "\n",
    "# Get model predictions\n",
    "train_y_pred = clf.predict(train_df[feature_keys])\n",
    "val_y_pred = clf.predict(val_df[feature_keys])\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(train_df[\"target_encoded\"], train_y_pred)\n",
    "val_accuracy = accuracy_score(val_df[\"target_encoded\"], val_y_pred)\n",
    "\n",
    "print(f\"Number of training samples: {len(train_df)}\")\n",
    "print(f\"Number of validation samples: {len(val_df)}\")\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ],
   "id": "499c78ead3bde306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Interactive holdout example\n",
    "# - What is the impact of changing the validation size?\n",
    "# - What is the impact of changing the random seed?\n",
    "# - What factors should be considered when selecting the validation size?\n",
    "from ml_nb_code import run_train_val_split_example\n",
    "run_train_val_split_example()"
   ],
   "id": "c77dd108584011d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Cross Validation Example\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from ml_nb_code import get_prepped_heart_df\n",
    "\n",
    "heart_df, feature_keys, _ = get_prepped_heart_df()\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Use cross-validation to evaluate the model\n",
    "cv_scores = cross_val_score(clf, heart_df[feature_keys], heart_df[\"target_encoded\"], cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f\"Cross-validation scores: {', '.join([f'{cur_score:.3f}' for cur_score in cv_scores])}\")\n",
    "print(f\"Mean cross-validation score: {cv_scores.mean():.2f}\")\n",
    "print(f\"Standard deviation of cross-validation scores: {cv_scores.std():.2f}\")"
   ],
   "id": "16e6fb586fd934a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Interactive cross validation example\n",
    "# - What is the impact of changing the number of folds?\n",
    "# - What is the impact of changing the random seed?\n",
    "# - What factors should be considered when selecting the number of folds?\n",
    "from ml_nb_code import run_cv_example\n",
    "run_cv_example()"
   ],
   "id": "e1051112e068cab5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "TODO: Add details on how to use cross-validation in combination with train-test split",
   "id": "a06f3cef19d305c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## How To Address Overfitting/Underfitting?\n",
    "\n",
    "### Overfitting\n",
    "- Collect more data\n",
    "- Reduce the complexity of the model\n",
    "- Regularization\n",
    "\n",
    "### Underfitting\n",
    "- Increase the complexity of the model\n",
    "- Different model type"
   ],
   "id": "ec42d85892c0e5f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters\n",
    "- Parameters that are not learned during training\n",
    "- Generally set before training the model\n",
    "- Can have a large impact on model performance, i.e. very useful for addressing overfitting/underfitting\n",
    "- Model type specific\n",
    "- Examples for a Decision Tree:\n",
    "    - Maximum depth of the tree\n",
    "    - Minimum number of samples required to split a node\n",
    "    - Minimum number of samples required at each leaf node\n",
    "    - For a full list see the Decision Tree [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
   ],
   "id": "82334cae868173bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Hyperparameter Example\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from ml_nb_code import get_prepped_heart_df\n",
    "seed = 42\n",
    "\n",
    "heart_df, features, label_encoder = get_prepped_heart_df()\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(heart_df[features],\n",
    "                                                  heart_df[\"target_encoded\"],\n",
    "                                                  test_size=0.2,\n",
    "                                                  random_state=seed)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "val_df = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=seed, max_depth=4)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(train_df[features], train_df[\"target_encoded\"])\n",
    "\n",
    "# Get model predictions\n",
    "train_y_pred = clf.predict(train_df[features])\n",
    "val_y_pred = clf.predict(val_df[features])\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(train_df[\"target_encoded\"], train_y_pred)\n",
    "val_accuracy = accuracy_score(val_df[\"target_encoded\"], val_y_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(14, 4))\n",
    "plot_tree(clf, filled=True, impurity=False, feature_names=features, class_names=label_encoder.inverse_transform(clf.classes_))\n",
    "plt.show()\n",
    "\n",
    "from ml_nb_code import plot_decision_boundary_heart\n",
    "plot_decision_boundary_heart(train_df, clf, features, val_df=val_df, figsize=(14, 4))"
   ],
   "id": "fe9161f1bba08608",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Interactive Hyperparameter Example\n",
    "### Task/Questions:\n",
    "# - What do these hyperparameters do?\n",
    "# - How do they impact the model?\n",
    "# - What are the differences and commonalities between the hyperparameters?\n",
    "# - This implementation uses a 80/20 train/validation split, how could this be improved?\n",
    "from ml_nb_code import hyperparam_tuning_example\n",
    "hyperparam_tuning_example()"
   ],
   "id": "f818f576baeacd6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## How to select the \"best\" hyperparameter values?\n",
    "\n",
    "### Hyperparameter tuning\n",
    "- Basic idea: Search through different hyperparameter values and evaluate the model performance\n",
    "- Lots of different methods available, see the [Scikit-learn User Guide](https://scikit-learn.org/stable/modules/grid_search.html) for more details\n",
    "\n",
    "##### Grid Search\n",
    "Determines the best hyperparameters by exhaustively searching through a specified parameter grid   \n",
    "\n",
    "##### Random Search\n",
    "Determines the best hyperparameters by randomly sampling from a specified parameter grid\n",
    "\n",
    "<img src=\"resources/imgs/ml_hyper_tuning.png\" width=\"1000\"/>"
   ],
   "id": "f1f4ebc9400b8ecb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Grid Search Example\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from ml_nb_code import get_prepped_heart_df\n",
    "\n",
    "heart_df, feature_keys, _ = get_prepped_heart_df()\n",
    "\n",
    "# Create the classifier\n",
    "rf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for the search\n",
    "param_grid = {\n",
    "    'max_depth': [2, 4, 8],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 5, 10, 20]\n",
    "}\n",
    "\n",
    "# Perform the grid search using cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=5, scoring='accuracy')\n",
    "grid_search.fit(heart_df[feature_keys], heart_df[\"target_encoded\"])\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ],
   "id": "6ae86de41a604edb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a dataframe with the results\n",
    "results = []\n",
    "for cur_param in param_grid.keys():\n",
    "    results.append(pd.DataFrame(grid_search.cv_results_)[f\"param_{cur_param}\"])\n",
    "\n",
    "results_df = pd.concat(results, axis=1)\n",
    "results_df[\"mean_test_score\"] = grid_search.cv_results_[\"mean_test_score\"]\n",
    "results_df[\"std_test_score\"] = grid_search.cv_results_[\"std_test_score\"]"
   ],
   "id": "2fdb71a37a97736e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "results_df.sort_values(\"mean_test_score\", ascending=False).head(10)",
   "id": "e71b4574f31470f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Full Hands-On Example\n",
    "- Add details here for a full hands on example\n",
    "TODO: \n",
    "- Determine which dataset to use for this"
   ],
   "id": "ebbe4ad0d0734501"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c4f8b52dfba4715d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
