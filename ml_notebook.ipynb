{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TODO:\n",
    "- Overview/Content of Notebook\n",
    "- How to use a notebook"
   ],
   "id": "25ed0b3b842e3af7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ML Overview",
   "id": "efb16ccb5af3dd58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing\n",
    "- Missing Data \n",
    "\t- Just mention this, don't go into detail. Very dataset specific, lots of good resources available.\n",
    "- Categorial Features\n",
    "- Label Encoding\n",
    "- Standardisation & Normalisation"
   ],
   "id": "cf51358b72c37b44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T01:11:27.478214Z",
     "start_time": "2024-08-01T01:11:27.475657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ],
   "id": "403eb88cec603552",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Missing Data\n",
    "- Important to check if there is missing data and handle this. Potential to break models (silently and non-silently).\n",
    "- Handling is dataset specific\n",
    "- Add resources links for this "
   ],
   "id": "1f526d43f05801d6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ml_nb_code import get_nan_example\n",
    "df = get_nan_example()\n",
    "df"
   ],
   "id": "f02f2bc42fc84f26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Drop rows with missing data\n",
    "df = df.dropna()\n",
    "df.loc[df.isna().any(axis=1)]"
   ],
   "id": "19aba4c4921f47b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df",
   "id": "17f35ad7121c4b6f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Standardisation & Normalisation\n",
    "\n",
    "Normalisation is important bla"
   ],
   "id": "bb5cd16ad9a25f36"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ml_nb_code import feature_scaling\n",
    "feature_scaling()"
   ],
   "id": "9402ce6976f04c3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Z-standardistaion: $\\frac{X - \\mu} {\\sigma}$",
   "id": "7afac742f0d8fc2b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "This makes the data have zero mean and unit variance",
   "id": "f897662933d3a0ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ml_nb_code import feature_scaling_example\n",
    "feature_scaling_example()"
   ],
   "id": "57bce32072d12cee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Can either use sklearn or do it manually",
   "id": "a25f45f235a48881"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Manually\n",
    "from ml_nb_code import get_fs_data\n",
    "df = get_fs_data()\n",
    "\n",
    "df[\"x1_norm\"] = (df[\"x1\"] - df[\"x1\"].mean()) / df[\"x1\"].std()\n",
    "df[\"x2_norm\"] = (df[\"x2\"] - df[\"x2\"].mean()) / df[\"x2\"].std()\n",
    "\n",
    "print(f\"Mean: {df['x1_norm'].mean()}, Std: {df['x1_norm'].std()}\")\n",
    "print(f\"Mean: {df['x2_norm'].mean()}, Std: {df['x2_norm'].std()}\")"
   ],
   "id": "5bda004f49a5880",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Using sklearn\n",
    "from ml_nb_code import get_fs_data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "df = get_fs_data()\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the data (i.e. calculate the mean and standard deviation)\n",
    "scaler.fit(df[[\"x1\", \"x2\"]])\n",
    "# Transform the data\n",
    "df[[\"x1_norm\", \"x2_norm\"]] = scaler.transform(df[[\"x1\", \"x2\"]])\n",
    "\n",
    "# Can combine the fit and transform steps\n",
    "# df[[\"x1_norm\", \"x2_norm\"]] = scaler.fit_transform(df[[\"x1\", \"x2\"]])\n",
    "\n",
    "print(f\"Mean: {df['x1_norm'].mean()}, Std: {df['x1_norm'].std()}\")\n",
    "print(f\"Mean: {df['x2_norm'].mean()}, Std: {df['x2_norm'].std()}\")"
   ],
   "id": "1900fac8bf0bb2f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Categorial Features/Inputs\n",
    "\n",
    "TODO: Change both categorial and label encoding to use the dataset used for the decision tree concepts.\n"
   ],
   "id": "2510eaf8c9a8daf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "df = pd.DataFrame({'Type': ['Resid', 'Comm', 'Indus', 'Resid', 'Indus', 'Comm']})\n",
    "df"
   ],
   "id": "2bda7fc2173f785d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "# Fit and transform the data\n",
    "encoded_data = encoder.fit_transform(df)\n",
    "\n",
    "# Create a new dataframe with the encoded data\n",
    "encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(['Type']), dtype=int)\n",
    "# Concatenate the original and encoded dataframes\n",
    "result_df = pd.concat([df, encoded_df], axis=1)\n",
    "result_df"
   ],
   "id": "53d8bfb3107e4c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Label encoding\n",
    "Similarly to model inputs, most model also require the target variable to be numerical. This is generally done using label encoding."
   ],
   "id": "8ba126cdda2bb3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "df = pd.DataFrame(data=np.arange(15).reshape(5, 3), columns=[\"Feature1\", \"Feature2\", \"Feature2\"])\n",
    "df[\"Target\"] = [\"Safe\", \"Unsafe\", \"Safe\", \"Safe\", \"Unsafe\"]\n",
    "df"
   ],
   "id": "fcc80877f4feda67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the data\n",
    "df[\"Target_encoded\"] = label_encoder.fit_transform(df[\"Target\"])\n",
    "df"
   ],
   "id": "5236da5f3d3d2d91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Inverse transform (do we need to show this here? or move to separate coding notebook)\n",
    "df[\"Target_2\"] = label_encoder.inverse_transform(df[\"Target_encoded\"])\n",
    "df"
   ],
   "id": "b806f278bf2a0360",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Fitting\n",
    "- Decision Tree\n",
    "    - Interactive example\n",
    "    - Sklearn example (Hands on)\n",
    "    - Visualisation   \n",
    "- Overfitting/Underfitting"
   ],
   "id": "d3f90c2619c3aa76"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Decision Tree Concepts",
   "id": "11b5ed885f5bf781"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### TODO: Check that this is working correctly?\n",
    "# From David Dempsey's notebook\n",
    "from ml_nb_code import decision_tree\n",
    "decision_tree()\n",
    "# TASK 1\n",
    "# move the top slider to divide the dataset, trying both features\n",
    "# try to separate the safe and unsafe bridges as much as possible\n",
    "# when you are satisfied with the split of data, check the box to lock the root node\n",
    "\n",
    "# TASK 2\n",
    "# repeat the exercise for the lefthand and righthand sliders below\n",
    "# further separate and subdivide the data, trying to distinguish the two binary classes\n",
    "# can you construct a decision tree that classifies the two bridge types based on their features?\n",
    "\n",
    "# Consider the original dataframe given in the cells above. Which part is the feature matrix X, and\n",
    "# which is the label vector y?\n",
    "# What are the parameters of this model? What are the hyperparameters?\n",
    "\n",
    "# TASK 3\n",
    "# Suppose you are given a new bridge: load_capacity of 45, steel, and 10 years old. What would your model predict?"
   ],
   "id": "b585b5ad169a47e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Descision Tree (Hands on)\n",
    "- Hands on data pre-processing of the heart disease dataset (todo: add details)\n",
    "- Train a decision tree classifier"
   ],
   "id": "f8c2278c03ba149c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Import relevant libraries and load the data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ml_nb_code import get_heart_df\n",
    "\n",
    "heart_df = get_heart_df(features=[\"thalach\", \"oldpeak\", \"thal\"])\n",
    "heart_df\n",
    "\n",
    "# from ml_nb_code import load_iris_df\n",
    "# iris_df, iris_feature_names = load_iris_df()\n",
    "# iris_df"
   ],
   "id": "79373e65cc9ec9c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## Hands-On - Prepare the data\n",
    "\n",
    "\n",
    "### Solution -- Hidden\n",
    "\n",
    "# Normalise the features\n",
    "numerical_features = [\"thalach\", \"oldpeak\"]\n",
    "std_scaler = StandardScaler()\n",
    "heart_df[numerical_features] = std_scaler.fit_transform(heart_df[numerical_features])\n",
    "\n",
    "# Encode the categorical features\n",
    "categorical_features = [\"thal\"]\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_data = encoder.fit_transform(heart_df[categorical_features])\n",
    "heart_df[encoder.get_feature_names_out()] = encoded_data\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "heart_df[\"target_encoded\"] = label_encoder.fit_transform(heart_df[\"target\"])\n",
    "\n",
    "features = numerical_features + list(encoder.get_feature_names_out())"
   ],
   "id": "5fa14e3d71184cb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "heart_df",
   "id": "1511cb94bfc7384b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit a decision tree classifier\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(heart_df[features], heart_df[\"target_encoded\"])\n",
    "\n",
    "# Get model predictions\n",
    "y_pred = clf.predict(heart_df[features])\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(heart_df[\"target_encoded\"], y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ],
   "id": "7a5d59b9f9a2ce5a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize the decision tree\n",
    "plt.figure(figsize=(10, 6))\n",
    "plot_tree(clf, filled=True, impurity=False, feature_names=features, class_names=label_encoder.inverse_transform(clf.classes_))\n",
    "plt.show()"
   ],
   "id": "462d71679450853c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Note: Left corresponds to True, Right corresponds to False",
   "id": "8968faeac046845"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot decision boundaries\n",
    "from ml_nb_code import plot_decision_boundary_heart\n",
    "plot_decision_boundary_heart(heart_df, clf, features)"
   ],
   "id": "431348a24a83fcb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Overfitting and Underfitting",
   "id": "5d71049b61a85eaf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ml_nb_code import linear_regression_fitting_example\n",
    "linear_regression_fitting_example()"
   ],
   "id": "ad252013a0bc7576",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Model Evaluation & Hyperparameters\n",
    "- Measure the models performance on unseen data (i.e. not used during training)\n",
    "- Commonly done by splitting avilable (labelled) data into a training and validation/testing set\n",
    "- Commonly 80/20 split\n",
    "\n",
    "#### Train/Validation Split\n",
    "- Training set: used to train the model\n",
    "- Validation set: used to evaluate the model\n",
    "\n",
    "TODO:\n",
    "- Add interactive example for both train/val split and cross validation to show effect of train/val proportion and number of folds\n"
   ],
   "id": "1835713521d9109c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T01:11:35.414885Z",
     "start_time": "2024-08-01T01:11:34.680863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Example\n",
    "### TODO:\n",
    "# - Find better dataset than iris, does not make the point particular well...\n",
    "#   - Needs to show overfitting of default decision tree\n",
    "# - Make interactive, show effect of validation data size\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ml_nb_code import get_prepped_heart_df\n",
    "\n",
    "heart_df, feature_keys = get_prepped_heart_df()\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(heart_df[feature_keys], heart_df[\"target_encoded\"], test_size=0.2, random_state=42)\n",
    "train_df = pd.concat([X_train, y_train], axis=1)\n",
    "val_df = pd.concat([X_val, y_val], axis=1)\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "clf.fit(train_df[feature_keys], train_df[\"target_encoded\"])\n",
    "\n",
    "# Get model predictions\n",
    "train_y_pred = clf.predict(train_df[feature_keys])\n",
    "val_y_pred = clf.predict(val_df[feature_keys])\n",
    "\n",
    "# Calculate accuracy\n",
    "train_accuracy = accuracy_score(train_df[\"target_encoded\"], train_y_pred)\n",
    "val_accuracy = accuracy_score(val_df[\"target_encoded\"], val_y_pred)\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")"
   ],
   "id": "7c0f4d9ae30b9b77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.97\n",
      "Validation Accuracy: 0.70\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "heart_df",
   "id": "71489ee7f5217609",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Cross Validation\n",
    "- Allows for more reliable model evaluation\n",
    "- Gives indication on uncertainty in training process\n",
    "- Todo: **Add schematic of how this works**"
   ],
   "id": "5f5166649bf23f91"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T01:20:21.516107Z",
     "start_time": "2024-08-01T01:20:20.036820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from ml_nb_code import get_prepped_heart_df\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "heart_df, feature_keys = get_prepped_heart_df()\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Use cross-validation to evaluate the model\n",
    "cv_scores = cross_val_score(clf, heart_df[feature_keys], heart_df[\"target_encoded\"], cv=5)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f\"Cross-validation scores: {', '.join([f'{cur_score:.3f}' for cur_score in cv_scores])}\")\n",
    "print(f\"Mean cross-validation score: {cv_scores.mean():.2f}\")\n",
    "print(f\"Standard deviation of cross-validation scores: {cv_scores.std():.2f}\")"
   ],
   "id": "16e6fb586fd934a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: 0.667, 0.700, 0.712, 0.593, 0.610\n",
      "Mean cross-validation score: 0.66\n",
      "Standard deviation of cross-validation scores: 0.05\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Hyperparameters\n",
    "- Parameters that are not learned during training\n",
    "- Often highly relevant for overfitting/underfitting\n",
    "    - E.g. Depth of Tree in Decision Tree"
   ],
   "id": "7a83c9ce0c39618c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T01:01:36.038579Z",
     "start_time": "2024-08-01T01:01:33.135077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### TODO: \n",
    "### - And decision boundary visualisation\n",
    "### - Details on what those parameters are\n",
    "### - Update this to use cross validation instead of train/test spli\n",
    "from ml_nb_code import hyperparam_tuning_example\n",
    "hyperparam_tuning_example()"
   ],
   "id": "c2efc698b1785412",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntSlider(value=10, description='Max Depth:', max=10, min=1, style=SliderStyle(description_widt…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c981838d280f48e1b277d87ba2ee3a8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f74df6e6d0d49ddbb039a473587ca4a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T00:44:48.697256Z",
     "start_time": "2024-08-01T00:44:48.689011Z"
    }
   },
   "cell_type": "code",
   "source": "heart_df",
   "id": "84e1ffdcaece09fe",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      thalach   oldpeak               thal       target  thal_Fixed_defect  \\\n",
       "0    0.017494  1.068965       Fixed_defect     Presence                1.0   \n",
       "1   -1.816334  0.381773             Normal  No Precense                0.0   \n",
       "2   -0.899420  1.326662  Reversable_defect  No Precense                0.0   \n",
       "3    1.633010  2.099753             Normal     Presence                0.0   \n",
       "4    0.978071  0.295874             Normal     Presence                0.0   \n",
       "..        ...       ...                ...          ...                ...   \n",
       "297 -1.161395 -0.734914  Reversable_defect  No Precense                0.0   \n",
       "298 -0.768432  0.124076  Reversable_defect  No Precense                0.0   \n",
       "299 -0.375469  2.013854  Reversable_defect  No Precense                0.0   \n",
       "300 -1.510696  0.124076  Reversable_defect  No Precense                0.0   \n",
       "301  1.065396 -0.906712             Normal  No Precense                0.0   \n",
       "\n",
       "     thal_Normal  thal_Reversable_defect  target_encoded  \n",
       "0            0.0                     0.0               1  \n",
       "1            1.0                     0.0               0  \n",
       "2            0.0                     1.0               0  \n",
       "3            1.0                     0.0               1  \n",
       "4            1.0                     0.0               1  \n",
       "..           ...                     ...             ...  \n",
       "297          0.0                     1.0               0  \n",
       "298          0.0                     1.0               0  \n",
       "299          0.0                     1.0               0  \n",
       "300          0.0                     1.0               0  \n",
       "301          1.0                     0.0               0  \n",
       "\n",
       "[297 rows x 8 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>thalach</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "      <th>thal_Fixed_defect</th>\n",
       "      <th>thal_Normal</th>\n",
       "      <th>thal_Reversable_defect</th>\n",
       "      <th>target_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017494</td>\n",
       "      <td>1.068965</td>\n",
       "      <td>Fixed_defect</td>\n",
       "      <td>Presence</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.816334</td>\n",
       "      <td>0.381773</td>\n",
       "      <td>Normal</td>\n",
       "      <td>No Precense</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.899420</td>\n",
       "      <td>1.326662</td>\n",
       "      <td>Reversable_defect</td>\n",
       "      <td>No Precense</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.633010</td>\n",
       "      <td>2.099753</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Presence</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978071</td>\n",
       "      <td>0.295874</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Presence</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>-1.161395</td>\n",
       "      <td>-0.734914</td>\n",
       "      <td>Reversable_defect</td>\n",
       "      <td>No Precense</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>-0.768432</td>\n",
       "      <td>0.124076</td>\n",
       "      <td>Reversable_defect</td>\n",
       "      <td>No Precense</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-0.375469</td>\n",
       "      <td>2.013854</td>\n",
       "      <td>Reversable_defect</td>\n",
       "      <td>No Precense</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-1.510696</td>\n",
       "      <td>0.124076</td>\n",
       "      <td>Reversable_defect</td>\n",
       "      <td>No Precense</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>1.065396</td>\n",
       "      <td>-0.906712</td>\n",
       "      <td>Normal</td>\n",
       "      <td>No Precense</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>297 rows × 8 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "fdf9977a47f5ac82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9affe8a381a42eec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hyperparameter Tuning\n",
    "- What is it\n",
    "\n",
    "TODO:\n",
    "- Add some visualisation for it\n",
    "- Example\n",
    "- Use all features?\n",
    "- Visualisation of results"
   ],
   "id": "f1f4ebc9400b8ecb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T01:57:45.931208Z",
     "start_time": "2024-08-01T01:57:42.095914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Grid Search Example\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from ml_nb_code import get_prepped_heart_df\n",
    "\n",
    "heart_df, feature_keys = get_prepped_heart_df()\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4, 10]\n",
    "}\n",
    "\n",
    "# Perform the grid search using cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=5, scoring='accuracy')\n",
    "grid_search.fit(heart_df[feature_keys], heart_df[\"target_encoded\"])\n",
    "\n",
    "# Print the best parameters and the best score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ],
   "id": "6ae86de41a604edb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best parameters: {'min_samples_leaf': 10, 'min_samples_split': 2, 'n_estimators': 200}\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T02:03:19.938214Z",
     "start_time": "2024-08-01T02:03:19.931229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a dataframe with the results\n",
    "results = []\n",
    "for cur_param in param_grid.keys():\n",
    "    results.append(pd.DataFrame(grid_search.cv_results_)[f\"param_{cur_param}\"])\n",
    "\n",
    "results_df = pd.concat(results, axis=1)\n",
    "results_df[\"mean_test_score\"] = grid_search.cv_results_[\"mean_test_score\"]"
   ],
   "id": "2fdb71a37a97736e",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T02:03:38.228828Z",
     "start_time": "2024-08-01T02:03:38.221537Z"
    }
   },
   "cell_type": "code",
   "source": "results_df.sort_values(\"mean_test_score\", ascending=False)",
   "id": "e71b4574f31470f2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   param_n_estimators param_min_samples_split param_min_samples_leaf  \\\n",
       "35                200                      10                     10   \n",
       "32                200                       5                     10   \n",
       "29                200                       2                     10   \n",
       "31                100                       5                     10   \n",
       "28                100                       2                     10   \n",
       "34                100                      10                     10   \n",
       "25                100                      10                      4   \n",
       "33                 50                      10                     10   \n",
       "30                 50                       5                     10   \n",
       "27                 50                       2                     10   \n",
       "24                 50                      10                      4   \n",
       "23                200                       5                      4   \n",
       "20                200                       2                      4   \n",
       "26                200                      10                      4   \n",
       "19                100                       2                      4   \n",
       "22                100                       5                      4   \n",
       "21                 50                       5                      4   \n",
       "18                 50                       2                      4   \n",
       "17                200                      10                      2   \n",
       "8                 200                      10                      1   \n",
       "7                 100                      10                      1   \n",
       "16                100                      10                      2   \n",
       "13                100                       5                      2   \n",
       "6                  50                      10                      1   \n",
       "15                 50                      10                      2   \n",
       "14                200                       5                      2   \n",
       "11                200                       2                      2   \n",
       "10                100                       2                      2   \n",
       "5                 200                       5                      1   \n",
       "0                  50                       2                      1   \n",
       "12                 50                       5                      2   \n",
       "4                 100                       5                      1   \n",
       "3                  50                       5                      1   \n",
       "9                  50                       2                      2   \n",
       "2                 200                       2                      1   \n",
       "1                 100                       2                      1   \n",
       "\n",
       "    mean_test_score  \n",
       "35         0.760791  \n",
       "32         0.760791  \n",
       "29         0.760791  \n",
       "31         0.757401  \n",
       "28         0.757401  \n",
       "34         0.757401  \n",
       "25         0.757288  \n",
       "33         0.754068  \n",
       "30         0.754068  \n",
       "27         0.754068  \n",
       "24         0.753955  \n",
       "23         0.753955  \n",
       "20         0.753955  \n",
       "26         0.750621  \n",
       "19         0.747288  \n",
       "22         0.747288  \n",
       "21         0.743898  \n",
       "18         0.743898  \n",
       "17         0.740452  \n",
       "8          0.737175  \n",
       "7          0.737175  \n",
       "16         0.737119  \n",
       "13         0.737006  \n",
       "6          0.733785  \n",
       "15         0.733729  \n",
       "14         0.733616  \n",
       "11         0.730339  \n",
       "10         0.727006  \n",
       "5          0.720282  \n",
       "0          0.720113  \n",
       "12         0.716949  \n",
       "4          0.716893  \n",
       "3          0.713503  \n",
       "9          0.713503  \n",
       "2          0.700000  \n",
       "1          0.700000  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_min_samples_leaf</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.760791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.760791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.760791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.757401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.757401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.757401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.757288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.754068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.754068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.754068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.753955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.753955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.753955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0.750621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.747288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.747288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.743898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.743898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.740452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>200</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.737175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.737175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.737119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.737006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.733785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0.733729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.733616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.730339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.727006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.720282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.720113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.716949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.716893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.713503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.713503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Full Hands-On Example\n",
    "- Add details here for a full hands on example\n",
    "TODO: \n",
    "- Determine which dataset to use for this"
   ],
   "id": "ebbe4ad0d0734501"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
